{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import L2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###load dataframes and specify the input column (to train the model on)\n",
    "ttrain=pd.read_pickle('./news_ttrain.pkl')\n",
    "tvalid=pd.read_pickle('./news_tvalid.pkl')\n",
    "test=pd.read_pickle('./news_test.pkl')\n",
    "input_col='cleaned_words'\n",
    "###Train a tokenizer and convert the input_col to integer sequences\n",
    "tokenizer=Tokenizer()\n",
    "train_list_str=ttrain[input_col].tolist()\n",
    "tokenizer.fit_on_texts(train_list_str)\n",
    "train_list = tokenizer.texts_to_sequences(train_list_str)\n",
    "###compute number of distinct words, the max length of a sequence, and pad sequences to be equal length \n",
    "tlmax_len=max([len(tlist) for tlist in train_list])\n",
    "tlnum_words=max([max(tlist) for tlist in train_list])\n",
    "train_list = pad_sequences(train_list, maxlen=tlmax_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74760d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tlmax_len, tlnum_words)\n",
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Now we preprocess the validation set, similar to the train_set\n",
    "valid_list_str=tvalid[input_col].tolist()\n",
    "valid_list = tokenizer.texts_to_sequences(valid_list_str)\n",
    "\n",
    "###need to truncate any strings that are too long, then pad to appropriate length\n",
    "valid_list = [tlist[0:tlmax_len] for tlist in valid_list]\n",
    "valid_list = pad_sequences(valid_list, maxlen=tlmax_len)\n",
    "\n",
    "###Same treatment for test set as the validation set\n",
    "test_list_str=test[input_col].tolist()\n",
    "test_list = tokenizer.texts_to_sequences(test_list_str)\n",
    "test_list = [tlist[0:tlmax_len] for tlist in test_list]\n",
    "test_list = pad_sequences(test_list, maxlen=tlmax_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_list.shape)\n",
    "print(valid_list.shape)\n",
    "print(test_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23743b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Now we make one hot encoded y vectors for the train and validation sets\n",
    "ytrain= pd.get_dummies(ttrain['category_col']).to_numpy()\n",
    "yvalid= pd.get_dummies(tvalid['category_col']).to_numpy()\n",
    "ytest= pd.get_dummies(test['category_col']).to_numpy()\n",
    "\n",
    "print(ytrain.shape, yvalid.shape, ytest.shape)\n",
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Some model hyperparameters, chosen by training different models over a set of options\n",
    "embed_len=256\n",
    "reg_param=0 ### in testing, even small regularization worsened accuracy\n",
    "lr=0.0002 ###smaller than default learning rate leads to more stable outcomes, albeit longer train times\n",
    "nfeatures=256\n",
    "\n",
    "### START OF MODEL ###\n",
    "input = Input(shape=tlmax_len)\n",
    "x = input\n",
    "x=Embedding(tlnum_words+1, embed_len, input_length=tlmax_len, mask_zero=True)(x)\n",
    "\n",
    "###Model creates 3 filters over kernels of size 2, 4, and 6. kernel arrangement chosen as the best from a list of tested options\n",
    "tower_1 = Conv1D(nfeatures, 2, padding='valid', activation='relu', kernel_regularizer=L2(reg_param))(x)\n",
    "tower_1 = GlobalMaxPooling1D() (tower_1)\n",
    "tower_2 = Conv1D(nfeatures, 4, padding='valid', activation='relu',kernel_regularizer=L2(reg_param))(x)\n",
    "tower_2 = GlobalMaxPooling1D() (tower_2)\n",
    "tower_3 = Conv1D(nfeatures, 6, padding='valid', activation='relu', kernel_regularizer=L2(reg_param))(x)\n",
    "tower_3 = GlobalMaxPooling1D() (tower_3)\n",
    "\n",
    "###Merge the output of the 3 filters, flatten, then combine their output with a dense softmax layer with dropout\n",
    "merged = concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "merged = Flatten()(merged)\n",
    "merged = Dropout(0.3)(merged)\n",
    "output=Dense(12, activation='softmax', kernel_regularizer=L2(reg_param))(merged)\n",
    "model=Model(inputs=input, outputs=output)\n",
    "###Compile model with custom choice of learning rate and binary crossentropy loss\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "### END OF MODEL ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa500a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', verbose=1, patience=1)\n",
    "out = model.fit(train_list, ytrain, validation_data=(valid_list, yvalid), epochs=3, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Generate prediction vector on the validation set \n",
    "categories = ['BLACK VOICES', 'BUSINESS', 'ENTERTAINMENT', 'FOOD & DRINK', 'PARENTING', 'POLITICS', 'QUEER VOICES', 'SPORTS', 'STYLE & BEAUTY', 'TRAVEL', 'WELLNESS', 'WORLD NEWS'] \n",
    "y_pred=model.predict(test_list).tolist()\n",
    "ypred1=[] ### vector of 0's and 1's\n",
    "ypred2=[] ### vector of corresponding string label\n",
    "for tlist in y_pred:\n",
    "    tmax=max(tlist)\n",
    "    temp=[1 if i==tmax else 0 for i in tlist]\n",
    "    for i, j in enumerate(temp):\n",
    "        if j==1:\n",
    "            ypred2.append(categories[i])\n",
    "    ypred1.append(temp)\n",
    "accuracy_score(ypred1, ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346666bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plotting a confusion matrix for the test set\n",
    "cm = confusion_matrix(test['category_col'].to_list(), ypred2)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "labels = categories\n",
    "class_names = labels\n",
    "\n",
    "# Plot confusion matrix in a beautiful manner\n",
    "fig = plt.figure(figsize=(16, 14))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "ax.xaxis.set_label_position('bottom')\n",
    "plt.xticks(rotation=90)\n",
    "ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "ax.xaxis.tick_bottom()\n",
    "\n",
    "ax.set_ylabel('True', fontsize=20)\n",
    "ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.title('Refined Confusion Matrix', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred1,target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "###save model predictions to the dataset for later analysis\n",
    "test['cnn_cat_pred']= ypred2\n",
    "test.to_pickle('./news_test_with_pred.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###some rows corresponding to failed model predictions are displayed here (I find them interesting)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "test[test['category_col']!=test['cnn_cat_pred']][['headline_col', 'short_description', 'cleaned_words', 'category_col', 'cnn_cat_pred']].head(50) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
